{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5131f095",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding (BPE) Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01397c25",
   "metadata": {},
   "source": [
    "### 2.1 The Unicode Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb7fc5d",
   "metadata": {},
   "source": [
    "**a. What Unicode character does chr(0) return?**\n",
    "\n",
    "'\\x00'\n",
    "\n",
    "**b. How does this character’s string representation (__repr__()) differ from its printed representation?**\n",
    "\n",
    "It printed as null.\n",
    "\n",
    "**c. What happens when this character occurs in text? It may be helpful to play around with the\n",
    "following in your Python interpreter and see if it matches your expectations:**\n",
    "\n",
    "```\n",
    ">>> chr(0)\n",
    "'\\x00'\n",
    ">>> chr(0).__repr__()\n",
    "\"'\\\\x00'\"\n",
    ">>> chr(0)\n",
    "'\\x00'\n",
    ">>> print(chr(0))\n",
    "\n",
    ">>> \"this is a test\" + chr(0) + \"string\"\n",
    "'this is a test\\x00string'\n",
    ">>> print(\"this is a test\" + chr(0) + \"string\")\n",
    "this is a teststring\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39c0d1",
   "metadata": {},
   "source": [
    "### 2.2 Unicode Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8c471",
   "metadata": {},
   "source": [
    "**a. What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than\n",
    "UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various\n",
    "input strings.**\n",
    "\n",
    "| Character | Description | UTF-8 (bytes)         | UTF-16 (bytes)        | UTF-32 (bytes)        |\n",
    "| --------- | ----------- | --------------------- | --------------------- | --------------------- |\n",
    "| `A`       | ASCII       | `0x41`                | `0x00 0x41`           | `0x00 0x00 0x00 0x41` |\n",
    "| `ñ`       | Latin-1     | `0xC3 0xB1`           | `0x00 0xF1`           | `0x00 0x00 0x00 0xF1` |\n",
    "| `中`       | CJK         | `0xE4 0xB8 0xAD`      | `0x4E 0x2D`           | `0x00 0x00 0x4E 0x2D` |\n",
    "| `😀`      | Emoji       | `0xF0 0x9F 0x98 0x80` | `0xD8 0x3D 0xDE 0x00` | `0x00 0x01 0xF6 0x00` |\n",
    " \n",
    "\n",
    "1. UTF-8 is more compact for common characters (ASCII)\n",
    "2. Each byte is in [0, 255] → fixed vocabulary size.\n",
    "\n",
    "**b. Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into\n",
    "a Unicode string. Why is this function incorrect? Provide an example of an input byte string\n",
    "that yields incorrect results.**\n",
    "\n",
    "```\n",
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "return \"\"\n",
    ".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    ">>> decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))\n",
    "'hello'\n",
    "```\n",
    "\n",
    "```\n",
    ">>> decode_utf8_bytes_to_str_wrong(\"中\".encode(\"utf-8\"))\n",
    "Traceback (most recent call last):\n",
    "  File \"<python-input-31>\", line 1, in <module>\n",
    "    decode_utf8_bytes_to_str_wrong(\"中\".encode(\"utf-8\"))\n",
    "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"<python-input-28>\", line 2, in decode_utf8_bytes_to_str_wrong\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "                    ~~~~~~~~~~~~~~~~~^^^^^^^^^\n",
    "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 0: unexpected end of data\n",
    "```\n",
    "\n",
    "The special characters need to be decoded through a sequence of bytes rather than one single byte.\n",
    "\n",
    "\n",
    "**c. Give a two byte sequence that does not decode to any Unicode character(s).**\n",
    "\n",
    "```\n",
    ">>> b'\\xe3\\xe4'.decode(\"utf-8\")\n",
    "Traceback (most recent call last):\n",
    "  File \"<python-input-32>\", line 1, in <module>\n",
    "    b'\\xe3\\xe4'.decode(\"utf-8\")\n",
    "    ~~~~~~~~~~~~~~~~~~^^^^^^^^^\n",
    "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe3 in position 0: invalid continuation byte\n",
    "```\n",
    "\n",
    "Missing the required 2 continuation bytes for 0xE3, and 0xE4 is not a valid follow-up byte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84efe162",
   "metadata": {},
   "source": [
    "### 2.5 Experimenting with BPE Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75b6558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nOnce upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things',\n",
       " ' like beautiful vases that were on display in a store. One day',\n",
       " ' Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  \\nHe said',\n",
       " ' “Wow',\n",
       " ' that is a really amazing vase! Can I buy it?” \\nThe shopkeeper smiled and said']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load TinyStories train data\n",
    "with open(\"./data/TinyStoriesV2-GPT4-train.txt\") as file:\n",
    "    words = file.read().split(',')\n",
    "    \n",
    "display(words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f47cae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use example util to find the chunk boundary which is required when doing the pre-tokenization in parallel\n",
    "import os\n",
    "from typing import BinaryIO\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO, \n",
    "    desired_num_chunks: int, \n",
    "    split_special_token: bytes\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), (\n",
    "        \"Must represent special token as a bytestring\"\n",
    "    )\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c57a029b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b96db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
