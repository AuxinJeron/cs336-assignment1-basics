{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9837b29",
   "metadata": {},
   "source": [
    "## Train BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "5aa404f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_bpe.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_bpe.py\n",
    "import os\n",
    "import regex as re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from typing import BinaryIO\n",
    "from collections import Counter, defaultdict\n",
    "from multiprocessing import Pool\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def train_bpe(\n",
    "    input_path: str | os.PathLike,\n",
    "    vocab_size: int,\n",
    "    special_tokens: list[str],\n",
    "    **kwargs,\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    # dict[int, bytes] => the tokenizer vacobulary, a mapping from int (token id) to bytes\n",
    "    # list[tuple[bytes, bytes]] => a list of BPE merges, each list (<token1>, <token2>) means token1 should be merged with token2\n",
    "    tokenizer_vocabs = dict()\n",
    "    merges = list()\n",
    "    num_processes = kwargs.get(\"num_processes\", os.cpu_count())\n",
    "    \n",
    "    # Initialize the vocabularies \n",
    "    # Append special tokens \n",
    "    for token in special_tokens:\n",
    "        tokenizer_vocabs[len(tokenizer_vocabs)] = token.encode(\"utf-8\")\n",
    "    # Append the 256 characters which can be represented by one byte\n",
    "    for i in range(256):\n",
    "        tokenizer_vocabs[len(tokenizer_vocabs)] = bytes([i])\n",
    "  \n",
    "    # Pre-tokenizations\n",
    "    pre_tokens = dict()\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        split_special_token = special_tokens[0] if special_tokens else \"<|endoftext|>\"\n",
    "        boundaries = find_chunk_boundaries(\n",
    "            f, num_processes, split_special_token.encode(\"utf-8\"))\n",
    "\n",
    "    # Prepare list of (input_path, start, end) arguments\n",
    "    chunk_args = [(input_path, start, end, special_tokens) for start, end in zip(boundaries[:-1], boundaries[1:])]\n",
    "    # Use multiprocessing.Pool\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        chunk_counters = pool.starmap(process_chunk_for_bpe, chunk_args)\n",
    "        \n",
    "    pre_tokens_counter = sum(chunk_counters, Counter())\n",
    "    pre_tokens = {k.encode(\"utf-8\") : v for k, v in pre_tokens_counter.items()}\n",
    "        \n",
    "    # Compute the merges\n",
    "    # Initialize token sequences\n",
    "    pre_token_sequences = defaultdict(list)\n",
    "    for pre_token, _ in pre_tokens.items():\n",
    "        for i in range(len(pre_token)):\n",
    "            pre_token_sequences[pre_token].append(bytes([pre_token[i]]))\n",
    "    \n",
    "    # Initialize the pair frequences\n",
    "    pair_freq = Counter()\n",
    "    token_ocurrences = defaultdict(set)\n",
    "    for pre_token, seq in pre_token_sequences.items():\n",
    "        for i in range(len(seq) - 1):\n",
    "            pair = (seq[i], seq[i + 1])\n",
    "            pair_freq[pair] += pre_tokens[pre_token]\n",
    "            token_ocurrences[seq[i]].add(pre_token)\n",
    "            token_ocurrences[seq[i + 1]].add(pre_token)\n",
    "            \n",
    "    total_iterations = vocab_size - len(tokenizer_vocabs.keys())\n",
    "    pbar = tqdm(total=total_iterations, desc=\"Merging\", ncols=70)\n",
    "    while len(tokenizer_vocabs.keys()) < vocab_size:\n",
    "        if len(pair_freq) == 0:\n",
    "            break\n",
    "        # Find the pair with highest frequency to create new token sequences\n",
    "        max_count = max(pair_freq.values())\n",
    "        most_frequent = max([k for k, v in pair_freq.items() if v == max_count])\n",
    "        \n",
    "        # Append the new merge\n",
    "        merges.append(most_frequent)\n",
    "        \n",
    "        # Add the new token in the tokenizer vocabularies \n",
    "        new_token = most_frequent[0] + most_frequent[1]\n",
    "        tokenizer_vocabs[len(tokenizer_vocabs)] = new_token\n",
    "        \n",
    "        # Set the frequencies of the merged pair tokens to 0\n",
    "        pair_freq[most_frequent] = 0\n",
    "        \n",
    "        # Update the token sequences of the impacted pre tokens \n",
    "        impacted_pre_tokens = token_ocurrences[most_frequent[0]] | token_ocurrences[most_frequent[1]]\n",
    "        for pre_token in impacted_pre_tokens:\n",
    "            seq = pre_token_sequences[pre_token]\n",
    "            # Rebuild the pre-token sequences for the impacted pre-token\n",
    "            i = 0\n",
    "            new_seq = list()\n",
    "            while i < len(seq):\n",
    "                if i < len(seq) - 1 and (seq[i], seq[i + 1]) == most_frequent:\n",
    "                    new_seq.append(new_token)\n",
    "                    token_ocurrences[new_token].add(pre_token)\n",
    "                    if i - 1 >= 0:\n",
    "                        pair_freq[(seq[i - 1], seq[i])] -= pre_tokens[pre_token]\n",
    "                        pair_freq[(seq[i - 1], new_token)] += pre_tokens[pre_token]\n",
    "                    if i + 2 < len(seq):\n",
    "                        pair_freq[(seq[i + 1], seq[i + 2])] -= pre_tokens[pre_token]\n",
    "                        pair_freq[(new_token, seq[i + 2])] += pre_tokens[pre_token]\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_seq.append(seq[i])\n",
    "                    i += 1\n",
    "            pre_token_sequences[pre_token] = new_seq\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    return tokenizer_vocabs, merges\n",
    "\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO, \n",
    "    desired_num_chunks: int, \n",
    "    split_special_token: bytes\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), (\n",
    "        \"Must represent special token as a bytestring\"\n",
    "    )\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))\n",
    "\n",
    "def process_chunk_for_bpe(input_path: str, start: int, end: int, special_tokens: list[str]) -> Counter:\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "        # escape special tokens first since `|` means differently in regex\n",
    "        escaped_tokens = [re.escape(tok) for tok in special_tokens]\n",
    "        # strip out special tokens\n",
    "        parts = re.split(\"|\".join(escaped_tokens), chunk)\n",
    "        result = Counter()\n",
    "        # NOTE: We should not do pre-tokenization across parts split by special tokens\n",
    "        for p in parts:\n",
    "            result += pre_tokenization(p)\n",
    "        return result\n",
    "\n",
    "\n",
    "def pre_tokenization(chunk) -> Counter:\n",
    "    PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    counter = Counter()\n",
    "    \n",
    "    for match in re.finditer(PAT, chunk):\n",
    "        word = match.group()\n",
    "        counter[word] += 1\n",
    "    \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "15698529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging: 100%|██████████████████████| 243/243 [00:02<00:00, 86.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({0: b'<|endoftext|>',\n",
       "  1: b'\\x00',\n",
       "  2: b'\\x01',\n",
       "  3: b'\\x02',\n",
       "  4: b'\\x03',\n",
       "  5: b'\\x04',\n",
       "  6: b'\\x05',\n",
       "  7: b'\\x06',\n",
       "  8: b'\\x07',\n",
       "  9: b'\\x08',\n",
       "  10: b'\\t',\n",
       "  11: b'\\n',\n",
       "  12: b'\\x0b',\n",
       "  13: b'\\x0c',\n",
       "  14: b'\\r',\n",
       "  15: b'\\x0e',\n",
       "  16: b'\\x0f',\n",
       "  17: b'\\x10',\n",
       "  18: b'\\x11',\n",
       "  19: b'\\x12',\n",
       "  20: b'\\x13',\n",
       "  21: b'\\x14',\n",
       "  22: b'\\x15',\n",
       "  23: b'\\x16',\n",
       "  24: b'\\x17',\n",
       "  25: b'\\x18',\n",
       "  26: b'\\x19',\n",
       "  27: b'\\x1a',\n",
       "  28: b'\\x1b',\n",
       "  29: b'\\x1c',\n",
       "  30: b'\\x1d',\n",
       "  31: b'\\x1e',\n",
       "  32: b'\\x1f',\n",
       "  33: b' ',\n",
       "  34: b'!',\n",
       "  35: b'\"',\n",
       "  36: b'#',\n",
       "  37: b'$',\n",
       "  38: b'%',\n",
       "  39: b'&',\n",
       "  40: b\"'\",\n",
       "  41: b'(',\n",
       "  42: b')',\n",
       "  43: b'*',\n",
       "  44: b'+',\n",
       "  45: b',',\n",
       "  46: b'-',\n",
       "  47: b'.',\n",
       "  48: b'/',\n",
       "  49: b'0',\n",
       "  50: b'1',\n",
       "  51: b'2',\n",
       "  52: b'3',\n",
       "  53: b'4',\n",
       "  54: b'5',\n",
       "  55: b'6',\n",
       "  56: b'7',\n",
       "  57: b'8',\n",
       "  58: b'9',\n",
       "  59: b':',\n",
       "  60: b';',\n",
       "  61: b'<',\n",
       "  62: b'=',\n",
       "  63: b'>',\n",
       "  64: b'?',\n",
       "  65: b'@',\n",
       "  66: b'A',\n",
       "  67: b'B',\n",
       "  68: b'C',\n",
       "  69: b'D',\n",
       "  70: b'E',\n",
       "  71: b'F',\n",
       "  72: b'G',\n",
       "  73: b'H',\n",
       "  74: b'I',\n",
       "  75: b'J',\n",
       "  76: b'K',\n",
       "  77: b'L',\n",
       "  78: b'M',\n",
       "  79: b'N',\n",
       "  80: b'O',\n",
       "  81: b'P',\n",
       "  82: b'Q',\n",
       "  83: b'R',\n",
       "  84: b'S',\n",
       "  85: b'T',\n",
       "  86: b'U',\n",
       "  87: b'V',\n",
       "  88: b'W',\n",
       "  89: b'X',\n",
       "  90: b'Y',\n",
       "  91: b'Z',\n",
       "  92: b'[',\n",
       "  93: b'\\\\',\n",
       "  94: b']',\n",
       "  95: b'^',\n",
       "  96: b'_',\n",
       "  97: b'`',\n",
       "  98: b'a',\n",
       "  99: b'b',\n",
       "  100: b'c',\n",
       "  101: b'd',\n",
       "  102: b'e',\n",
       "  103: b'f',\n",
       "  104: b'g',\n",
       "  105: b'h',\n",
       "  106: b'i',\n",
       "  107: b'j',\n",
       "  108: b'k',\n",
       "  109: b'l',\n",
       "  110: b'm',\n",
       "  111: b'n',\n",
       "  112: b'o',\n",
       "  113: b'p',\n",
       "  114: b'q',\n",
       "  115: b'r',\n",
       "  116: b's',\n",
       "  117: b't',\n",
       "  118: b'u',\n",
       "  119: b'v',\n",
       "  120: b'w',\n",
       "  121: b'x',\n",
       "  122: b'y',\n",
       "  123: b'z',\n",
       "  124: b'{',\n",
       "  125: b'|',\n",
       "  126: b'}',\n",
       "  127: b'~',\n",
       "  128: b'\\x7f',\n",
       "  129: b'\\x80',\n",
       "  130: b'\\x81',\n",
       "  131: b'\\x82',\n",
       "  132: b'\\x83',\n",
       "  133: b'\\x84',\n",
       "  134: b'\\x85',\n",
       "  135: b'\\x86',\n",
       "  136: b'\\x87',\n",
       "  137: b'\\x88',\n",
       "  138: b'\\x89',\n",
       "  139: b'\\x8a',\n",
       "  140: b'\\x8b',\n",
       "  141: b'\\x8c',\n",
       "  142: b'\\x8d',\n",
       "  143: b'\\x8e',\n",
       "  144: b'\\x8f',\n",
       "  145: b'\\x90',\n",
       "  146: b'\\x91',\n",
       "  147: b'\\x92',\n",
       "  148: b'\\x93',\n",
       "  149: b'\\x94',\n",
       "  150: b'\\x95',\n",
       "  151: b'\\x96',\n",
       "  152: b'\\x97',\n",
       "  153: b'\\x98',\n",
       "  154: b'\\x99',\n",
       "  155: b'\\x9a',\n",
       "  156: b'\\x9b',\n",
       "  157: b'\\x9c',\n",
       "  158: b'\\x9d',\n",
       "  159: b'\\x9e',\n",
       "  160: b'\\x9f',\n",
       "  161: b'\\xa0',\n",
       "  162: b'\\xa1',\n",
       "  163: b'\\xa2',\n",
       "  164: b'\\xa3',\n",
       "  165: b'\\xa4',\n",
       "  166: b'\\xa5',\n",
       "  167: b'\\xa6',\n",
       "  168: b'\\xa7',\n",
       "  169: b'\\xa8',\n",
       "  170: b'\\xa9',\n",
       "  171: b'\\xaa',\n",
       "  172: b'\\xab',\n",
       "  173: b'\\xac',\n",
       "  174: b'\\xad',\n",
       "  175: b'\\xae',\n",
       "  176: b'\\xaf',\n",
       "  177: b'\\xb0',\n",
       "  178: b'\\xb1',\n",
       "  179: b'\\xb2',\n",
       "  180: b'\\xb3',\n",
       "  181: b'\\xb4',\n",
       "  182: b'\\xb5',\n",
       "  183: b'\\xb6',\n",
       "  184: b'\\xb7',\n",
       "  185: b'\\xb8',\n",
       "  186: b'\\xb9',\n",
       "  187: b'\\xba',\n",
       "  188: b'\\xbb',\n",
       "  189: b'\\xbc',\n",
       "  190: b'\\xbd',\n",
       "  191: b'\\xbe',\n",
       "  192: b'\\xbf',\n",
       "  193: b'\\xc0',\n",
       "  194: b'\\xc1',\n",
       "  195: b'\\xc2',\n",
       "  196: b'\\xc3',\n",
       "  197: b'\\xc4',\n",
       "  198: b'\\xc5',\n",
       "  199: b'\\xc6',\n",
       "  200: b'\\xc7',\n",
       "  201: b'\\xc8',\n",
       "  202: b'\\xc9',\n",
       "  203: b'\\xca',\n",
       "  204: b'\\xcb',\n",
       "  205: b'\\xcc',\n",
       "  206: b'\\xcd',\n",
       "  207: b'\\xce',\n",
       "  208: b'\\xcf',\n",
       "  209: b'\\xd0',\n",
       "  210: b'\\xd1',\n",
       "  211: b'\\xd2',\n",
       "  212: b'\\xd3',\n",
       "  213: b'\\xd4',\n",
       "  214: b'\\xd5',\n",
       "  215: b'\\xd6',\n",
       "  216: b'\\xd7',\n",
       "  217: b'\\xd8',\n",
       "  218: b'\\xd9',\n",
       "  219: b'\\xda',\n",
       "  220: b'\\xdb',\n",
       "  221: b'\\xdc',\n",
       "  222: b'\\xdd',\n",
       "  223: b'\\xde',\n",
       "  224: b'\\xdf',\n",
       "  225: b'\\xe0',\n",
       "  226: b'\\xe1',\n",
       "  227: b'\\xe2',\n",
       "  228: b'\\xe3',\n",
       "  229: b'\\xe4',\n",
       "  230: b'\\xe5',\n",
       "  231: b'\\xe6',\n",
       "  232: b'\\xe7',\n",
       "  233: b'\\xe8',\n",
       "  234: b'\\xe9',\n",
       "  235: b'\\xea',\n",
       "  236: b'\\xeb',\n",
       "  237: b'\\xec',\n",
       "  238: b'\\xed',\n",
       "  239: b'\\xee',\n",
       "  240: b'\\xef',\n",
       "  241: b'\\xf0',\n",
       "  242: b'\\xf1',\n",
       "  243: b'\\xf2',\n",
       "  244: b'\\xf3',\n",
       "  245: b'\\xf4',\n",
       "  246: b'\\xf5',\n",
       "  247: b'\\xf6',\n",
       "  248: b'\\xf7',\n",
       "  249: b'\\xf8',\n",
       "  250: b'\\xf9',\n",
       "  251: b'\\xfa',\n",
       "  252: b'\\xfb',\n",
       "  253: b'\\xfc',\n",
       "  254: b'\\xfd',\n",
       "  255: b'\\xfe',\n",
       "  256: b'\\xff',\n",
       "  257: b' t',\n",
       "  258: b'he',\n",
       "  259: b' a',\n",
       "  260: b' s',\n",
       "  261: b' w',\n",
       "  262: b' the',\n",
       "  263: b'nd',\n",
       "  264: b'ed',\n",
       "  265: b' b',\n",
       "  266: b' to',\n",
       "  267: b' and',\n",
       "  268: b' h',\n",
       "  269: b' f',\n",
       "  270: b' T',\n",
       "  271: b'in',\n",
       "  272: b' wa',\n",
       "  273: b're',\n",
       "  274: b'it',\n",
       "  275: b'ou',\n",
       "  276: b' l',\n",
       "  277: b' d',\n",
       "  278: b' c',\n",
       "  279: b' p',\n",
       "  280: b'ay',\n",
       "  281: b' m',\n",
       "  282: b'er',\n",
       "  283: b' was',\n",
       "  284: b' The',\n",
       "  285: b'om',\n",
       "  286: b' he',\n",
       "  287: b'is',\n",
       "  288: b' n',\n",
       "  289: b'im',\n",
       "  290: b'ar',\n",
       "  291: b'on',\n",
       "  292: b' sa',\n",
       "  293: b'll',\n",
       "  294: b'id',\n",
       "  295: b' ha',\n",
       "  296: b' g',\n",
       "  297: b'at',\n",
       "  298: b' S',\n",
       "  299: b'ing',\n",
       "  300: b'ot',\n",
       "  301: b'en',\n",
       "  302: b'an',\n",
       "  303: b'le',\n",
       "  304: b'or',\n",
       "  305: b'ir',\n",
       "  306: b'am',\n",
       "  307: b'et',\n",
       "  308: b' H',\n",
       "  309: b' it',\n",
       "  310: b' th',\n",
       "  311: b'ig',\n",
       "  312: b' They',\n",
       "  313: b' pl',\n",
       "  314: b' in',\n",
       "  315: b'il',\n",
       "  316: b' He',\n",
       "  317: b' \"',\n",
       "  318: b'ow',\n",
       "  319: b'ver',\n",
       "  320: b'ri',\n",
       "  321: b' u',\n",
       "  322: b'ut',\n",
       "  323: b' play',\n",
       "  324: b'ith',\n",
       "  325: b' said',\n",
       "  326: b' be',\n",
       "  327: b' day',\n",
       "  328: b' with',\n",
       "  329: b'pp',\n",
       "  330: b'On',\n",
       "  331: b' o',\n",
       "  332: b' y',\n",
       "  333: b'oo',\n",
       "  334: b'ked',\n",
       "  335: b' r',\n",
       "  336: b' her',\n",
       "  337: b'ce',\n",
       "  338: b'ld',\n",
       "  339: b' his',\n",
       "  340: b' Tim',\n",
       "  341: b' I',\n",
       "  342: b' She',\n",
       "  343: b' st',\n",
       "  344: b'ke',\n",
       "  345: b' e',\n",
       "  346: b' big',\n",
       "  347: b'nt',\n",
       "  348: b'ck',\n",
       "  349: b'very',\n",
       "  350: b' you',\n",
       "  351: b'st',\n",
       "  352: b've',\n",
       "  353: b'end',\n",
       "  354: b'un',\n",
       "  355: b' happ',\n",
       "  356: b' on',\n",
       "  357: b'all',\n",
       "  358: b'riend',\n",
       "  359: b' friend',\n",
       "  360: b' they',\n",
       "  361: b' L',\n",
       "  362: b'ily',\n",
       "  363: b' we',\n",
       "  364: b' had',\n",
       "  365: b' up',\n",
       "  366: b' li',\n",
       "  367: b' not',\n",
       "  368: b'her',\n",
       "  369: b' want',\n",
       "  370: b'itt',\n",
       "  371: b' of',\n",
       "  372: b'ad',\n",
       "  373: b' do',\n",
       "  374: b' B',\n",
       "  375: b'se',\n",
       "  376: b' happy',\n",
       "  377: b'ent',\n",
       "  378: b' very',\n",
       "  379: b' M',\n",
       "  380: b'es',\n",
       "  381: b' saw',\n",
       "  382: b'One',\n",
       "  383: b' that',\n",
       "  384: b'ould',\n",
       "  385: b\"'s\",\n",
       "  386: b' for',\n",
       "  387: b'ittle',\n",
       "  388: b' mom',\n",
       "  389: b' little',\n",
       "  390: b' so',\n",
       "  391: b' sh',\n",
       "  392: b' she',\n",
       "  393: b'ime',\n",
       "  394: b'ch',\n",
       "  395: b' nam',\n",
       "  396: b' time',\n",
       "  397: b' k',\n",
       "  398: b' ne',\n",
       "  399: b'ound',\n",
       "  400: b'.\"',\n",
       "  401: b' there',\n",
       "  402: b' named',\n",
       "  403: b' bo',\n",
       "  404: b' sm',\n",
       "  405: b' were',\n",
       "  406: b' wanted',\n",
       "  407: b' Lily',\n",
       "  408: b' friends',\n",
       "  409: b'out',\n",
       "  410: b'ird',\n",
       "  411: b' but',\n",
       "  412: b'ved',\n",
       "  413: b'ht',\n",
       "  414: b'!\"',\n",
       "  415: b'The',\n",
       "  416: b' Tom',\n",
       "  417: b' bird',\n",
       "  418: b'el',\n",
       "  419: b'ake',\n",
       "  420: b' an',\n",
       "  421: b'al',\n",
       "  422: b' too',\n",
       "  423: b'ome',\n",
       "  424: b' went',\n",
       "  425: b' wh',\n",
       "  426: b'ide',\n",
       "  427: b'Once',\n",
       "  428: b' all',\n",
       "  429: b' It',\n",
       "  430: b' hel',\n",
       "  431: b'ug',\n",
       "  432: b'ue',\n",
       "  433: b' help',\n",
       "  434: b' is',\n",
       "  435: b' loo',\n",
       "  436: b' A',\n",
       "  437: b' upon',\n",
       "  438: b' lo',\n",
       "  439: b'ter',\n",
       "  440: b'ry',\n",
       "  441: b' toy',\n",
       "  442: b'ore',\n",
       "  443: b' fun',\n",
       "  444: b'ind',\n",
       "  445: b'get',\n",
       "  446: b'ill',\n",
       "  447: b'ame',\n",
       "  448: b' as',\n",
       "  449: b' j',\n",
       "  450: b'ra',\n",
       "  451: b' at',\n",
       "  452: b'gether',\n",
       "  453: b' cat',\n",
       "  454: b' did',\n",
       "  455: b' re',\n",
       "  456: b'ur',\n",
       "  457: b' together',\n",
       "  458: b'ack',\n",
       "  459: b' se',\n",
       "  460: b'ly',\n",
       "  461: b' tre',\n",
       "  462: b'ood',\n",
       "  463: b' dog',\n",
       "  464: b'ic',\n",
       "  465: b' could',\n",
       "  466: b'ted',\n",
       "  467: b' can',\n",
       "  468: b' ball',\n",
       "  469: b'ard',\n",
       "  470: b' gir',\n",
       "  471: b' their',\n",
       "  472: b'ark',\n",
       "  473: b' girl',\n",
       "  474: b' played',\n",
       "  475: b'ec',\n",
       "  476: b'my',\n",
       "  477: b' him',\n",
       "  478: b' go',\n",
       "  479: b'way',\n",
       "  480: b' ro',\n",
       "  481: b'?\"',\n",
       "  482: b'hed',\n",
       "  483: b'ain',\n",
       "  484: b' kn',\n",
       "  485: b' le',\n",
       "  486: b' out',\n",
       "  487: b'hen',\n",
       "  488: b' are',\n",
       "  489: b' fr',\n",
       "  490: b'um',\n",
       "  491: b\"'t\",\n",
       "  492: b' them',\n",
       "  493: b' boy',\n",
       "  494: b' sad',\n",
       "  495: b'ul',\n",
       "  496: b'ax',\n",
       "  497: b' tree',\n",
       "  498: b'other',\n",
       "  499: b'oug'},\n",
       " [(b' ', b't'),\n",
       "  (b'h', b'e'),\n",
       "  (b' ', b'a'),\n",
       "  (b' ', b's'),\n",
       "  (b' ', b'w'),\n",
       "  (b' t', b'he'),\n",
       "  (b'n', b'd'),\n",
       "  (b'e', b'd'),\n",
       "  (b' ', b'b'),\n",
       "  (b' t', b'o'),\n",
       "  (b' a', b'nd'),\n",
       "  (b' ', b'h'),\n",
       "  (b' ', b'f'),\n",
       "  (b' ', b'T'),\n",
       "  (b'i', b'n'),\n",
       "  (b' w', b'a'),\n",
       "  (b'r', b'e'),\n",
       "  (b'i', b't'),\n",
       "  (b'o', b'u'),\n",
       "  (b' ', b'l'),\n",
       "  (b' ', b'd'),\n",
       "  (b' ', b'c'),\n",
       "  (b' ', b'p'),\n",
       "  (b'a', b'y'),\n",
       "  (b' ', b'm'),\n",
       "  (b'e', b'r'),\n",
       "  (b' wa', b's'),\n",
       "  (b' T', b'he'),\n",
       "  (b'o', b'm'),\n",
       "  (b' ', b'he'),\n",
       "  (b'i', b's'),\n",
       "  (b' ', b'n'),\n",
       "  (b'i', b'm'),\n",
       "  (b'a', b'r'),\n",
       "  (b'o', b'n'),\n",
       "  (b' s', b'a'),\n",
       "  (b'l', b'l'),\n",
       "  (b'i', b'd'),\n",
       "  (b' h', b'a'),\n",
       "  (b' ', b'g'),\n",
       "  (b'a', b't'),\n",
       "  (b' ', b'S'),\n",
       "  (b'in', b'g'),\n",
       "  (b'o', b't'),\n",
       "  (b'e', b'n'),\n",
       "  (b'a', b'n'),\n",
       "  (b'l', b'e'),\n",
       "  (b'o', b'r'),\n",
       "  (b'i', b'r'),\n",
       "  (b'a', b'm'),\n",
       "  (b'e', b't'),\n",
       "  (b' ', b'H'),\n",
       "  (b' ', b'it'),\n",
       "  (b' t', b'h'),\n",
       "  (b'i', b'g'),\n",
       "  (b' The', b'y'),\n",
       "  (b' p', b'l'),\n",
       "  (b' ', b'in'),\n",
       "  (b'i', b'l'),\n",
       "  (b' H', b'e'),\n",
       "  (b' ', b'\"'),\n",
       "  (b'o', b'w'),\n",
       "  (b'v', b'er'),\n",
       "  (b'r', b'i'),\n",
       "  (b' ', b'u'),\n",
       "  (b'u', b't'),\n",
       "  (b' pl', b'ay'),\n",
       "  (b'it', b'h'),\n",
       "  (b' sa', b'id'),\n",
       "  (b' b', b'e'),\n",
       "  (b' d', b'ay'),\n",
       "  (b' w', b'ith'),\n",
       "  (b'p', b'p'),\n",
       "  (b'O', b'n'),\n",
       "  (b' ', b'o'),\n",
       "  (b' ', b'y'),\n",
       "  (b'o', b'o'),\n",
       "  (b'k', b'ed'),\n",
       "  (b' ', b'r'),\n",
       "  (b' he', b'r'),\n",
       "  (b'c', b'e'),\n",
       "  (b'l', b'd'),\n",
       "  (b' h', b'is'),\n",
       "  (b' T', b'im'),\n",
       "  (b' ', b'I'),\n",
       "  (b' S', b'he'),\n",
       "  (b' s', b't'),\n",
       "  (b'k', b'e'),\n",
       "  (b' ', b'e'),\n",
       "  (b' b', b'ig'),\n",
       "  (b'n', b't'),\n",
       "  (b'c', b'k'),\n",
       "  (b'ver', b'y'),\n",
       "  (b' y', b'ou'),\n",
       "  (b's', b't'),\n",
       "  (b'v', b'e'),\n",
       "  (b'e', b'nd'),\n",
       "  (b'u', b'n'),\n",
       "  (b' ha', b'pp'),\n",
       "  (b' ', b'on'),\n",
       "  (b'a', b'll'),\n",
       "  (b'ri', b'end'),\n",
       "  (b' f', b'riend'),\n",
       "  (b' the', b'y'),\n",
       "  (b' ', b'L'),\n",
       "  (b'il', b'y'),\n",
       "  (b' w', b'e'),\n",
       "  (b' ha', b'd'),\n",
       "  (b' u', b'p'),\n",
       "  (b' l', b'i'),\n",
       "  (b' n', b'ot'),\n",
       "  (b'he', b'r'),\n",
       "  (b' wa', b'nt'),\n",
       "  (b'it', b't'),\n",
       "  (b' o', b'f'),\n",
       "  (b'a', b'd'),\n",
       "  (b' d', b'o'),\n",
       "  (b' ', b'B'),\n",
       "  (b's', b'e'),\n",
       "  (b' happ', b'y'),\n",
       "  (b'en', b't'),\n",
       "  (b' ', b'very'),\n",
       "  (b' ', b'M'),\n",
       "  (b'e', b's'),\n",
       "  (b' sa', b'w'),\n",
       "  (b'On', b'e'),\n",
       "  (b' th', b'at'),\n",
       "  (b'ou', b'ld'),\n",
       "  (b\"'\", b's'),\n",
       "  (b' f', b'or'),\n",
       "  (b'itt', b'le'),\n",
       "  (b' m', b'om'),\n",
       "  (b' l', b'ittle'),\n",
       "  (b' s', b'o'),\n",
       "  (b' s', b'h'),\n",
       "  (b' s', b'he'),\n",
       "  (b'im', b'e'),\n",
       "  (b'c', b'h'),\n",
       "  (b' n', b'am'),\n",
       "  (b' t', b'ime'),\n",
       "  (b' ', b'k'),\n",
       "  (b' n', b'e'),\n",
       "  (b'ou', b'nd'),\n",
       "  (b'.', b'\"'),\n",
       "  (b' the', b're'),\n",
       "  (b' nam', b'ed'),\n",
       "  (b' b', b'o'),\n",
       "  (b' s', b'm'),\n",
       "  (b' we', b're'),\n",
       "  (b' want', b'ed'),\n",
       "  (b' L', b'ily'),\n",
       "  (b' friend', b's'),\n",
       "  (b'ou', b't'),\n",
       "  (b'ir', b'd'),\n",
       "  (b' b', b'ut'),\n",
       "  (b'v', b'ed'),\n",
       "  (b'h', b't'),\n",
       "  (b'!', b'\"'),\n",
       "  (b'T', b'he'),\n",
       "  (b' T', b'om'),\n",
       "  (b' b', b'ird'),\n",
       "  (b'e', b'l'),\n",
       "  (b'a', b'ke'),\n",
       "  (b' a', b'n'),\n",
       "  (b'a', b'l'),\n",
       "  (b' to', b'o'),\n",
       "  (b'om', b'e'),\n",
       "  (b' w', b'ent'),\n",
       "  (b' w', b'h'),\n",
       "  (b'id', b'e'),\n",
       "  (b'On', b'ce'),\n",
       "  (b' a', b'll'),\n",
       "  (b' I', b't'),\n",
       "  (b' he', b'l'),\n",
       "  (b'u', b'g'),\n",
       "  (b'u', b'e'),\n",
       "  (b' hel', b'p'),\n",
       "  (b' ', b'is'),\n",
       "  (b' l', b'oo'),\n",
       "  (b' ', b'A'),\n",
       "  (b' up', b'on'),\n",
       "  (b' l', b'o'),\n",
       "  (b't', b'er'),\n",
       "  (b'r', b'y'),\n",
       "  (b' to', b'y'),\n",
       "  (b'o', b're'),\n",
       "  (b' f', b'un'),\n",
       "  (b'i', b'nd'),\n",
       "  (b'g', b'et'),\n",
       "  (b'i', b'll'),\n",
       "  (b'am', b'e'),\n",
       "  (b' a', b's'),\n",
       "  (b' ', b'j'),\n",
       "  (b'r', b'a'),\n",
       "  (b' a', b't'),\n",
       "  (b'get', b'her'),\n",
       "  (b' c', b'at'),\n",
       "  (b' d', b'id'),\n",
       "  (b' ', b're'),\n",
       "  (b'u', b'r'),\n",
       "  (b' to', b'gether'),\n",
       "  (b'a', b'ck'),\n",
       "  (b' s', b'e'),\n",
       "  (b'l', b'y'),\n",
       "  (b' t', b're'),\n",
       "  (b'oo', b'd'),\n",
       "  (b' do', b'g'),\n",
       "  (b'i', b'c'),\n",
       "  (b' c', b'ould'),\n",
       "  (b't', b'ed'),\n",
       "  (b' c', b'an'),\n",
       "  (b' b', b'all'),\n",
       "  (b'ar', b'd'),\n",
       "  (b' g', b'ir'),\n",
       "  (b' the', b'ir'),\n",
       "  (b'ar', b'k'),\n",
       "  (b' gir', b'l'),\n",
       "  (b' play', b'ed'),\n",
       "  (b'e', b'c'),\n",
       "  (b'm', b'y'),\n",
       "  (b' h', b'im'),\n",
       "  (b' g', b'o'),\n",
       "  (b'w', b'ay'),\n",
       "  (b' r', b'o'),\n",
       "  (b'?', b'\"'),\n",
       "  (b'he', b'd'),\n",
       "  (b'a', b'in'),\n",
       "  (b' k', b'n'),\n",
       "  (b' l', b'e'),\n",
       "  (b' ', b'out'),\n",
       "  (b'he', b'n'),\n",
       "  (b' a', b're'),\n",
       "  (b' f', b'r'),\n",
       "  (b'u', b'm'),\n",
       "  (b\"'\", b't'),\n",
       "  (b' the', b'm'),\n",
       "  (b' bo', b'y'),\n",
       "  (b' sa', b'd'),\n",
       "  (b'u', b'l'),\n",
       "  (b'a', b'x'),\n",
       "  (b' tre', b'e'),\n",
       "  (b'ot', b'her'),\n",
       "  (b'ou', b'g')])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_bpe import train_bpe\n",
    "\n",
    "result = train_bpe(\"/home/ec2-user/SageMaker/stanford-cs336/cs336-assignment1-basics/data/TinyStoriesV2-GPT4-valid.txt\",\n",
    "         500,\n",
    "         [\"<|endoftext|>\"])\n",
    "\n",
    "result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "6708c1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TestBar:   0%|                                            | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "TestBar:  20%|███████▏                            | 1/5 [00:00<00:02,  2.00it/s]\u001b[A\n",
      "TestBar:  40%|██████████████▍                     | 2/5 [00:01<00:01,  1.99it/s]\u001b[A\n",
      "TestBar:  60%|█████████████████████▌              | 3/5 [00:01<00:01,  1.99it/s]\u001b[A\n",
      "TestBar:  80%|████████████████████████████▊       | 4/5 [00:02<00:00,  1.99it/s]\u001b[A\n",
      "TestBar: 100%|████████████████████████████████████| 5/5 [00:02<00:00,  1.99it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "with tqdm(total=5, desc=\"TestBar\", ncols=80) as pbar:\n",
    "    for _ in range(5):\n",
    "        time.sleep(0.5)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b91c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
