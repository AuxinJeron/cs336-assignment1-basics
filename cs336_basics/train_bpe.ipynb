{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9837b29",
   "metadata": {},
   "source": [
    "## Train BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa404f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_bpe.py\n",
    "import os\n",
    "import regex as re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from typing import BinaryIO\n",
    "from collections import Counter, defaultdict\n",
    "from multiprocessing import Pool\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def train_bpe(\n",
    "    input_path: str | os.PathLike,\n",
    "    vocab_size: int,\n",
    "    special_tokens: list[str],\n",
    "    **kwargs,\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    # dict[int, bytes] => the tokenizer vacobulary, a mapping from int (token id) to bytes\n",
    "    # list[tuple[bytes, bytes]] => a list of BPE merges, each list (<token1>, <token2>) means token1 should be merged with token2\n",
    "    tokenizer_vocabs = dict()\n",
    "    merges = list()\n",
    "    num_processes = kwargs.get(\"num_processes\", os.cpu_count())\n",
    "    \n",
    "    # Initialize the vocabularies \n",
    "    # Append special tokens \n",
    "    for token in special_tokens:\n",
    "        tokenizer_vocabs[len(tokenizer_vocabs)] = token.encode(\"utf-8\")\n",
    "    # Append the 256 characters which can be represented by one byte\n",
    "    for i in range(256):\n",
    "        tokenizer_vocabs[len(tokenizer_vocabs)] = bytes([i])\n",
    "  \n",
    "    # Pre-tokenizations\n",
    "    pre_tokens = dict()\n",
    "    logging.info(\"Chunking...\")\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        split_special_token = special_tokens[0] if special_tokens else \"<|endoftext|>\"\n",
    "        boundaries = find_chunk_boundaries(\n",
    "            f, num_processes, split_special_token.encode(\"utf-8\"))\n",
    "\n",
    "    # Prepare list of (input_path, start, end) arguments\n",
    "    chunk_args = [(input_path, start, end, special_tokens) for start, end in zip(boundaries[:-1], boundaries[1:])]\n",
    "    # Use multiprocessing.Pool\n",
    "    logging.info(\"Pre-tokenization...\")\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        chunk_counters = pool.starmap(process_chunk_for_bpe, chunk_args)\n",
    "        \n",
    "    pre_tokens_counter = sum(chunk_counters, Counter())\n",
    "    pre_tokens = {k.encode(\"utf-8\") : v for k, v in pre_tokens_counter.items()}\n",
    "        \n",
    "    # Compute the merges\n",
    "    # Initialize token sequences\n",
    "    pre_token_sequences = defaultdict(list)\n",
    "    for pre_token, _ in pre_tokens.items():\n",
    "        for i in range(len(pre_token)):\n",
    "            pre_token_sequences[pre_token].append(bytes([pre_token[i]]))\n",
    "    \n",
    "    # Initialize the pair frequences\n",
    "    pair_freq = Counter()\n",
    "    token_ocurrences = defaultdict(set)\n",
    "    for pre_token, seq in pre_token_sequences.items():\n",
    "        for i in range(len(seq) - 1):\n",
    "            pair = (seq[i], seq[i + 1])\n",
    "            pair_freq[pair] += pre_tokens[pre_token]\n",
    "            token_ocurrences[seq[i]].add(pre_token)\n",
    "            token_ocurrences[seq[i + 1]].add(pre_token)\n",
    "            \n",
    "    total_iterations = vocab_size - len(tokenizer_vocabs.keys())\n",
    "    pbar = tqdm(total=total_iterations, desc=\"Merging\", ncols=70)\n",
    "    while len(tokenizer_vocabs.keys()) < vocab_size:\n",
    "        if len(pair_freq) == 0:\n",
    "            break\n",
    "        # Find the pair with highest frequency to create new token sequences\n",
    "        max_count = max(pair_freq.values())\n",
    "        most_frequent = max([k for k, v in pair_freq.items() if v == max_count])\n",
    "        \n",
    "        # Append the new merge\n",
    "        merges.append(most_frequent)\n",
    "        \n",
    "        # Add the new token in the tokenizer vocabularies \n",
    "        new_token = most_frequent[0] + most_frequent[1]\n",
    "        tokenizer_vocabs[len(tokenizer_vocabs)] = new_token\n",
    "        \n",
    "        # Set the frequencies of the merged pair tokens to 0\n",
    "        pair_freq[most_frequent] = 0\n",
    "        \n",
    "        # Update the token sequences of the impacted pre tokens \n",
    "        impacted_pre_tokens = token_ocurrences[most_frequent[0]] | token_ocurrences[most_frequent[1]]\n",
    "        for pre_token in impacted_pre_tokens:\n",
    "            seq = pre_token_sequences[pre_token]\n",
    "            # Rebuild the pre-token sequences for the impacted pre-token\n",
    "            i = 0\n",
    "            new_seq = list()\n",
    "            while i < len(seq):\n",
    "                if i < len(seq) - 1 and (seq[i], seq[i + 1]) == most_frequent:\n",
    "                    new_seq.append(new_token)\n",
    "                    token_ocurrences[new_token].add(pre_token)\n",
    "                    if i - 1 >= 0:\n",
    "                        pair_freq[(seq[i - 1], seq[i])] -= pre_tokens[pre_token]\n",
    "                        pair_freq[(seq[i - 1], new_token)] += pre_tokens[pre_token]\n",
    "                    if i + 2 < len(seq):\n",
    "                        pair_freq[(seq[i + 1], seq[i + 2])] -= pre_tokens[pre_token]\n",
    "                        pair_freq[(new_token, seq[i + 2])] += pre_tokens[pre_token]\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_seq.append(seq[i])\n",
    "                    i += 1\n",
    "            pre_token_sequences[pre_token] = new_seq\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    return tokenizer_vocabs, merges\n",
    "\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO, \n",
    "    desired_num_chunks: int, \n",
    "    split_special_token: bytes\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), (\n",
    "        \"Must represent special token as a bytestring\"\n",
    "    )\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))\n",
    "\n",
    "def process_chunk_for_bpe(input_path: str, start: int, end: int, special_tokens: list[str]) -> Counter:\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "        # escape special tokens first since `|` means differently in regex\n",
    "        escaped_tokens = [re.escape(tok) for tok in special_tokens]\n",
    "        # strip out special tokens\n",
    "        parts = re.split(\"|\".join(escaped_tokens), chunk)\n",
    "        result = Counter()\n",
    "        # NOTE: We should not do pre-tokenization across parts split by special tokens\n",
    "        for p in parts:\n",
    "            result += pre_tokenization(p)\n",
    "        return result\n",
    "\n",
    "\n",
    "def pre_tokenization(chunk) -> Counter:\n",
    "    PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    counter = Counter()\n",
    "    \n",
    "    for match in re.finditer(PAT, chunk):\n",
    "        word = match.group()\n",
    "        counter[word] += 1\n",
    "    \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "15698529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 05:06:06,911 - INFO - Chunking...\n",
      "2025-07-17 05:06:06,912 - INFO - Pre-tokenization...\n",
      "Merging: 100%|██████████████████████| 243/243 [00:03<00:00, 75.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.45 s, sys: 48.3 ms, total: 3.5 s\n",
      "Wall time: 6.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_bpe import train_bpe\n",
    "\n",
    "result = train_bpe(\"/home/ec2-user/SageMaker/stanford-cs336/cs336-assignment1-basics/data/TinyStoriesV2-GPT4-valid.txt\",\n",
    "         500,\n",
    "         [\"<|endoftext|>\"])\n",
    "\n",
    "result\n",
    "\n",
    "vocab_str = {k: str(v) for k, v in bpe_tinystories_result[0].items()}\n",
    "merges_str = [(str(a), str(b)) for a, b in bpe_tinystories_result[1]]\n",
    "\n",
    "with open('test_result.json', 'w') as f:\n",
    "    json.dump({'vocab': vocab_str, 'merges': merges_str}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708c1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 05:35:14,555 - INFO - Chunking...\n",
      "2025-07-17 05:35:14,557 - INFO - Pre-tokenization...\n",
      "Merging: 100%|████████████████████| 9743/9743 [03:19<00:00, 48.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 20s, sys: 484 ms, total: 3min 21s\n",
      "Wall time: 20min 9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train_bpe_tinystories\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_bpe import train_bpe\n",
    "import json\n",
    "\n",
    "bpe_tinystories_result = train_bpe(\"/home/ec2-user/SageMaker/stanford-cs336/cs336-assignment1-basics/data/TinyStoriesV2-GPT4-train.txt\",\n",
    "         10000,\n",
    "         [\"<|endoftext|>\"])\n",
    "\n",
    "vocab_str = {k: str(v) for k, v in bpe_tinystories_result[0].items()}\n",
    "merges_str = [(str(a), str(b)) for a, b in bpe_tinystories_result[1]]\n",
    "\n",
    "with open('bpe_tinystories_result.json', 'w') as f:\n",
    "    json.dump({'vocab': vocab_str, 'merges': merges_str}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b91c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 05:55:25,050 - INFO - Chunking...\n",
      "2025-07-17 05:55:25,060 - INFO - Pre-tokenization...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train_bpe_expts_owt\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train_bpe import train_bpe\n",
    "import json\n",
    "\n",
    "bpe_expts_owt_result = train_bpe(\"/home/ec2-user/SageMaker/stanford-cs336/cs336-assignment1-basics/data/owt_train.txt\",\n",
    "         32000,\n",
    "         [\"<|endoftext|>\"])\n",
    "\n",
    "# bytes to hex for safe serialization\n",
    "vocab_str = {k: str(v) for k, v in bpe_expts_owt_result[0].items()}\n",
    "merges_str = [(str(a), str(b)) for a, b in bpe_expts_owt_result[1]]\n",
    "\n",
    "with open('bpe_expts_owt_result.json', 'w') as f:\n",
    "    json.dump({'vocab': vocab_str, 'merges': merges_str}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a5a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
